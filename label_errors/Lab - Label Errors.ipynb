{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwfWqPeA1zX0"
   },
   "source": [
    "# Lab â€” Label Errors\n",
    "\n",
    "This lab highlights data-centric AI techniques (using [confident learning](https://jair.org/index.php/jair/article/view/12125)) to improve the accuracy of an XGBoost classifier on a noisy dataset that has label errors.\n",
    "\n",
    "The DCAI techniques demonstrated in this lab involve optimizing the dataset itself rather than altering the model's architecture or hyperparameters. As a result, it is possible to achieve further improvements in accuracy by fine-tuning the model in conjunction with the newly enhanced data, but that is not the focus of this lab.\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "- Establish a baseline [XGBoost](https://xgboost.readthedocs.io/) model accuracy on the original data\n",
    "- Automatically find mislabeled data points by:\n",
    "    - Computing out-of-sample predicted probabilities\n",
    "    - Estimating the number of label errors using confident learning\n",
    "    - Ranking errors, using the number of label errors as a cutoff in identifying issues\n",
    "- Remove the bad data\n",
    "- Retrain the exact same XGBoost model to see the improvement in test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software installation\n",
    "\n",
    "This lab relies on a couple PyPI packages. If you don't have them installed, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (1.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (1.5.3)\n",
      "Requirement already satisfied: cleanlab in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: scipy in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from xgboost==1.7) (1.10.1)\n",
      "Requirement already satisfied: numpy in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from xgboost==1.7) (1.24.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tqdm>=4.53.0 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from cleanlab) (4.65.0)\n",
      "Requirement already satisfied: termcolor>=2.0.0 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from cleanlab) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sebastian/repos/dcai-lab/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/sebastian/repos/dcai-lab/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.7 scikit-learn pandas cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je7P55z4RwX_"
   },
   "source": [
    "## Setup and Data Processing\n",
    "\n",
    "Let's take a look at the dataset used in this lab, a tabular dataset of student grades.\n",
    "\n",
    "The data includes three exam scores (numerical features), a written note (categorical feature with missing values), and a (noisy) letter grade (categorical label). Our aim is to train a model to classify the grade for each student based on the other features.\n",
    "\n",
    "In this dataset, 20% of the grade labels are actually incorrect (the `noisy_letter_grade` column). Synthetic noise was added to this dataset for the purpose of this lab. In this lab, we have access to the true letter grade each student should have received (the `letter_grade` column), which we use for evaluating both the underlying accuracy of model predictions and how well our approach detects which data are mislabeled. We are careful to only use these true grades for evaluation, not for model training.\n",
    "\n",
    "In the real world, you don't have access to the true labels (you only observe the `noisy_letter_grade`, not the true `letter_grade`). So when evaluating models in the real world, you have to be careful to make sure that your test set is free of error (using methods like those covered in this lab, ideally combined with human review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nQVmMBQOS43j",
    "outputId": "54a45659-ecb6-47fe-acfa-d0424027af47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f48f73</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bd4e7</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1795d</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb9d7a</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9acca4</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3                    notes letter_grade  \\\n",
       "0  f48f73      53      77      93                      NaN            C   \n",
       "1  0bd4e7      81      64      80  great participation +10            B   \n",
       "2  e1795d      74      88      97                      NaN            B   \n",
       "3  cb9d7a      61      94      78                      NaN            C   \n",
       "4  9acca4      48      90      91                      NaN            C   \n",
       "\n",
       "  noisy_letter_grade  \n",
       "0                  C  \n",
       "1                  B  \n",
       "2                  B  \n",
       "3                  C  \n",
       "4                  C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./student-grades.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f48f73</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bd4e7</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1795d</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb9d7a</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9acca4</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3 notes  letter_grade  noisy_letter_grade\n",
       "0  f48f73      53      77      93     5             2                   2\n",
       "1  0bd4e7      81      64      80     2             1                   1\n",
       "2  e1795d      74      88      97     5             1                   1\n",
       "3  cb9d7a      61      94      78     5             2                   2\n",
       "4  9acca4      48      90      91     5             2                   2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = df.copy()\n",
    "# Transform letter grades and notes to categorical numbers.\n",
    "# Necessary for XGBoost.\n",
    "df['letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['letter_grade'])\n",
    "df['noisy_letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['noisy_letter_grade'])\n",
    "df['notes'] = preprocessing.LabelEncoder().fit_transform(df[\"notes\"])\n",
    "df['notes'] = df['notes'].astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVH_iciASD9F"
   },
   "source": [
    "# Get What We Need\n",
    "\n",
    "To apply confident learning (the technique explained in today's lecture), we need to obtain [**out-of-sample** predicted probabilities](https://docs.cleanlab.ai/stable/tutorials/pred_probs_cross_val.html#out-of-sample-predicted-probabilities) for all of our data. To do this, we can use K-fold cross validation: for each fold, we will train on some subset of our data and get predictions on the rest of the data that was _not_ used for training.\n",
    "\n",
    "We need to choose a model in order to do this. For this lab, we'll use [XGBoost](https://xgboost.readthedocs.io/), a library implementing gradient-boosted decision trees, a class of model commonly used for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCS19IqJsQUL",
    "outputId": "fd89b945-6793-4b3a-a017-7b19f8e6a29b"
   },
   "outputs": [],
   "source": [
    "# Prepare training data (remove labels from the dataframe) and labels\n",
    "data = df.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "labels = df['noisy_letter_grade']\n",
    "\n",
    "# XGBoost(experimental) supports categorical data.\n",
    "# Here we use default hyperparameters for simplicity.\n",
    "# Get out-of-sample predicted probabilities and check model accuracy.\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: getting out-of-sample predicted probabilities\n",
    "\n",
    "Compute out-of-sample predicted probabilities for every data point. You can do this manually using for loops and multiple invocations of model training and prediction, or you can use scikit-learn's [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) (if you're using this function, take a look at the documentations, and in particular, the `method=` keyword argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "data      exam_1  exam_2  exam_3 notes\n",
      "0        53      77      93     5\n",
      "1        81      64      80     2\n",
      "2        74      88      97     5\n",
      "3        61      94      78     5\n",
      "4        48      90      91     5\n",
      "..      ...     ...     ...   ...\n",
      "936      37      83      69     4\n",
      "937      59      37      75     4\n",
      "938       0      97      60     0\n",
      "939      74      80      91     5\n",
      "940      88      66      74     5\n",
      "\n",
      "[941 rows x 4 columns]\n",
      "labels 0      2\n",
      "1      1\n",
      "2      1\n",
      "3      2\n",
      "4      2\n",
      "      ..\n",
      "936    4\n",
      "937    4\n",
      "938    4\n",
      "939    1\n",
      "940    2\n",
      "Name: noisy_letter_grade, Length: 941, dtype: int64\n",
      "pred_probs.shape (941, 5)\n",
      "pred_probs [[9.9915704e-03 1.0754441e-02 9.5985240e-01 5.0153048e-03 1.4386256e-02]\n",
      " [4.8501577e-02 8.6473954e-01 1.3487166e-03 8.9849411e-03 7.6425239e-02]\n",
      " [7.7535305e-03 9.5749086e-01 1.9698440e-04 9.6462184e-04 3.3594064e-02]\n",
      " ...\n",
      " [1.8168031e-01 4.3267220e-01 4.6972525e-03 3.4395212e-01 3.6998145e-02]\n",
      " [3.5881605e-02 9.0193230e-01 1.5249980e-02 1.4933412e-03 4.5442801e-02]\n",
      " [8.2281864e-01 2.2477485e-02 1.1010921e-01 2.8254291e-02 1.6340349e-02]]\n"
     ]
    }
   ],
   "source": [
    "# pred_probs should be a Nx5 matrix of out-of-sample predicted probabilities, with N = len(data)\n",
    "print(\"model\", model)\n",
    "print(\"data\", data)\n",
    "print(\"labels\", labels)\n",
    "\n",
    "pred_probs = cross_val_predict(model, X=data, y=labels, method=\"predict_proba\")\n",
    "print(\"pred_probs.shape\", pred_probs.shape)\n",
    "print(\"pred_probs\", pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model accuracy on original data\n",
    "\n",
    "Now that we have out-of-sample predicted probabilities, we can also check the model's (cross-val) accuracy on the original (noisy) data, so we'll have a baseline to compare our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 67.4%\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(pred_probs, axis=1)\n",
    "acc_original = accuracy_score(preds, labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding label issues automatically\n",
    "\n",
    "We count label issues using confident learning. First, we need to compute class thresholds for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: computing class thresholds\n",
    "\n",
    "Implement the Confident Learning algorithm for computing class thresholds for the 5 classes. You can refer to slide 26 from today's lecture or see equation 2 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The class threshold for each class is the model's expected (average) self-confidence for each class. In other words, to compute the threshold for a particular class, you can average the predicted probability for that class, for all datapoints that are labeled with that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_class_thresholds(pred_probs: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    assert len(pred_probs) == len(labels)\n",
    "    label_sums = defaultdict(float)\n",
    "    label_counts = defaultdict(int)\n",
    "    for i in range(len(pred_probs)):\n",
    "        label_sums[labels[i]] += pred_probs[i][labels[i]]\n",
    "        label_counts[labels[i]] += 1\n",
    "    class_thresholds = np.zeros(pred_probs.shape[1])\n",
    "    for label_index in range(pred_probs.shape[1]):\n",
    "        class_thresholds[label_index] = label_sums[label_index] / label_counts[label_index]\n",
    "        \n",
    "    return class_thresholds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_thresholds [0.72064708 0.64154055 0.67945672 0.47879468 0.45895547]\n",
      "test_thresholds [0.15 0.35]\n"
     ]
    }
   ],
   "source": [
    "# should be a numpy array of length 5\n",
    "thresholds = compute_class_thresholds(pred_probs, labels.to_numpy())\n",
    "print(\"class_thresholds\", thresholds)\n",
    "test_pred_probs = np.array([[0.1, 0.5],[0.3, 0.4],[0.4,0.3],[0.2,0.2]])\n",
    "test_labels = np.array([0,1,1,0])\n",
    "expected_thresholds = np.array([0.15, 0.35])\n",
    "test_thresholds = compute_class_thresholds(test_pred_probs, test_labels)\n",
    "np.testing.assert_almost_equal(expected_thresholds, test_thresholds)\n",
    "print(\"test_thresholds\", test_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: constructing the confident joint\n",
    "\n",
    "Next, we compute the confident joint, a matrix that counts the number of label errors for each noisy label $\\tilde{y}$ and true label $y^*$. You can follow the algorithm that we walked through in slide 27 from today's lecture, or see equation 1 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The confident joint C is a K x K matrix (with K = 5 for this dataset), where `C[i][j]` is an estimate of the count of the number of data points with noisy label `i` and true label `j`. From lecture, recall that we put a data point in bin `(i, j)` if its given label is `i`, and its predicted probability for class `j` is above the threshold for class `j` (`thresholds[j]`). Each data point should only go in a single bin; if a data point's predicted probability is above the class threshold for multiple classes, it goes in the bin for which it has the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confident_joint(pred_probs: np.ndarray, labels: np.ndarray, thresholds: np.ndarray) -> np.ndarray:\n",
    "    assert len(pred_probs) == len(labels)\n",
    "    assert len(thresholds) == pred_probs.shape[1]\n",
    "    confident_joint = np.zeros((5,5), dtype=np.int32)\n",
    "    for i in range(len(pred_probs)):\n",
    "        over_threshold = pred_probs[i] >= thresholds\n",
    "        if np.sum(over_threshold) >= 1:\n",
    "            largest_index_over_threshold = -1\n",
    "            largest_prob = -1\n",
    "            for label_i, is_over in enumerate(over_threshold):\n",
    "                if is_over and pred_probs[i][label_i] >= largest_prob:\n",
    "                    largest_index_over_threshold = label_i\n",
    "            assert largest_index_over_threshold >= 0\n",
    "            confident_joint[largest_index_over_threshold][labels[i]] += 1\n",
    "    return confident_joint\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confident_joint [[157   4   2   3  19]\n",
      " [  4 145   4  37  16]\n",
      " [ 18   1 103   0  16]\n",
      " [  8  42   5  76   8]\n",
      " [ 14  11   8  10  70]]\n"
     ]
    }
   ],
   "source": [
    "C = compute_confident_joint(pred_probs, labels.to_numpy(), thresholds)\n",
    "print(\"confident_joint\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: count the number of label issues\n",
    "\n",
    "Now that we have the confident joint C, we can count the estimated number of label issues in our dataset. Recall that this is the sum of the off-diagonal entries (the cases where we estimate that a label has been flipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_label_issues(confident_joint: np.ndarray):\n",
    "    assert confident_joint.shape[0] == confident_joint.shape[1]\n",
    "    c_copy = np.copy(confident_joint)\n",
    "    for i in range(confident_joint.shape[0]):\n",
    "        c_copy[i][i] = 0\n",
    "    return np.sum(c_copy)\n",
    "\n",
    "num_label_issues = count_number_of_label_issues(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated noise rate: 24.4%\n"
     ]
    }
   ],
   "source": [
    "print('Estimated noise rate: {:.1f}%'.format(100*num_label_issues / pred_probs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: filter out label issues\n",
    "\n",
    "In this lab, our approach to identifying issues is to rank the data points by a score (\"self-confidence\", the model's predicted probability for a data point's given label) and then take the top `num_label_issues` of those.\n",
    "\n",
    "First, we want to compute the model's _self-confidence_ for each data point. For a data point `i`, that is `pred_probs[i, labels[i]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_confidences [9.59852397e-01 8.64739537e-01 9.57490861e-01 7.92189837e-01\n",
      " 5.84582150e-01 2.15417743e-01 9.01733458e-01 9.72999692e-01\n",
      " 7.22036898e-01 7.76102304e-01 5.93638957e-01 2.75136041e-03\n",
      " 9.78547931e-02 8.03519607e-01 1.85576864e-02 9.08367217e-01\n",
      " 9.27756608e-01 7.21474141e-02 4.83085122e-03 9.14228141e-01\n",
      " 9.61366594e-01 6.49827421e-01 9.36279118e-01 1.13028067e-03\n",
      " 4.96614307e-01 4.16250169e-01 8.78703296e-01 4.65151248e-03\n",
      " 9.58725870e-01 8.92124474e-01 6.79981411e-01 1.39547154e-01\n",
      " 1.35882571e-01 8.18970919e-01 8.38480890e-01 9.22731757e-01\n",
      " 5.12100640e-04 1.45897344e-01 2.40159377e-01 9.61963117e-01\n",
      " 2.67440616e-03 5.19209743e-01 3.94767821e-01 9.99574006e-01\n",
      " 3.47918510e-01 4.33425233e-02 5.33589423e-01 9.63702679e-01\n",
      " 9.52003896e-01 1.06611378e-01 9.12769973e-01 6.45021260e-01\n",
      " 4.03165042e-01 9.77456272e-01 8.92511904e-01 9.49116200e-02\n",
      " 1.39909824e-02 3.97332199e-02 9.47127715e-02 6.75032556e-01\n",
      " 8.26852560e-01 6.40993536e-01 3.07677202e-02 9.74599540e-01\n",
      " 9.94876444e-01 2.37767259e-03 6.33323908e-01 8.44468653e-01\n",
      " 5.34423947e-01 8.90319608e-03 6.49179041e-01 1.23676739e-03\n",
      " 8.66955996e-01 2.21216287e-02 5.14696717e-01 5.54755509e-01\n",
      " 9.96047676e-01 6.68149710e-01 5.27044237e-01 9.24990356e-01\n",
      " 9.81479228e-01 8.79612565e-01 6.56173944e-01 5.57839056e-04\n",
      " 7.07617700e-01 8.32697272e-01 7.15229332e-01 9.96648371e-01\n",
      " 9.96800184e-01 6.28677249e-01 9.38490689e-01 2.60395885e-01\n",
      " 3.44806939e-01 9.82766151e-01 8.14500630e-01 9.92704928e-01\n",
      " 9.95244920e-01 9.54075336e-01 9.43044066e-01 9.58784521e-01\n",
      " 8.91857922e-01 9.67164099e-01 4.83576149e-01 9.99314785e-01\n",
      " 9.53418851e-01 3.82881016e-02 9.96600211e-01 9.80065167e-01\n",
      " 1.67420805e-01 7.74032116e-01 9.85043287e-01 4.25864607e-01\n",
      " 8.17670166e-01 6.31202996e-01 3.18215519e-01 8.71447265e-01\n",
      " 8.93257618e-01 3.27782661e-01 8.50742877e-01 9.94743109e-01\n",
      " 3.23070399e-02 9.55415487e-01 8.99462625e-02 9.87865865e-01\n",
      " 8.10939372e-01 9.93993461e-01 9.76420045e-01 9.21581507e-01\n",
      " 9.27474499e-01 6.31294260e-03 1.55312046e-02 3.68673533e-01\n",
      " 9.52895403e-01 6.34610653e-01 3.11071724e-01 4.91462827e-01\n",
      " 9.66456234e-01 4.95901816e-02 5.88015504e-02 4.19223960e-03\n",
      " 1.51140511e-01 9.73644435e-01 9.45117712e-01 2.67170649e-02\n",
      " 4.64583328e-03 9.01833653e-01 9.51771855e-01 8.93684983e-01\n",
      " 9.77510393e-01 1.51869625e-01 2.75230467e-01 9.53233361e-01\n",
      " 4.43783373e-01 9.83453333e-01 3.10577780e-01 9.78900671e-01\n",
      " 9.80596304e-01 9.60136950e-01 1.48778306e-02 1.17013685e-03\n",
      " 1.45501390e-01 3.00030671e-02 9.45989907e-01 9.80268538e-01\n",
      " 9.61167753e-01 9.71672475e-01 6.83406413e-01 7.51070499e-01\n",
      " 9.64393616e-01 4.27698065e-03 9.79566634e-01 9.94576097e-01\n",
      " 3.41193110e-01 8.75504978e-04 1.77556816e-02 8.36857021e-01\n",
      " 9.60397243e-01 9.70929444e-01 2.26061597e-01 7.84466229e-03\n",
      " 9.90606010e-01 2.04598933e-01 3.38401943e-01 9.93772209e-01\n",
      " 9.16618202e-03 9.54407454e-01 5.92145808e-02 2.75818825e-01\n",
      " 8.57021630e-01 9.45515513e-01 1.45906657e-01 9.50870812e-01\n",
      " 8.90728831e-01 6.78584754e-01 9.16534424e-01 1.14437558e-01\n",
      " 7.82735050e-01 8.35034028e-02 2.16632113e-02 1.84363239e-02\n",
      " 9.01464939e-01 9.76196945e-01 9.79243040e-01 3.01250041e-01\n",
      " 9.92458224e-01 9.90947545e-01 1.23189315e-01 7.58950174e-01\n",
      " 8.03764723e-03 9.63913560e-01 3.01388595e-02 7.95284152e-01\n",
      " 1.79125667e-01 9.94915962e-01 7.53485262e-01 9.96000350e-01\n",
      " 7.53569186e-01 1.75143123e-01 8.59375179e-01 4.72251475e-02\n",
      " 9.52992857e-01 1.20692715e-01 9.90223825e-01 2.61034548e-01\n",
      " 1.17815780e-02 2.24616267e-02 4.94113892e-01 3.61531943e-01\n",
      " 9.16619956e-01 9.07465041e-01 1.05122291e-02 9.43154335e-01\n",
      " 6.78409457e-01 1.81069195e-01 8.93146038e-01 9.94007587e-01\n",
      " 9.92832541e-01 9.54550505e-01 1.46754319e-02 9.69854832e-01\n",
      " 9.95048583e-01 9.47250962e-01 9.63669956e-01 8.72857749e-01\n",
      " 9.98961449e-01 6.20124578e-01 9.67481673e-01 9.20148253e-01\n",
      " 7.59867668e-01 9.97683167e-01 9.24706459e-01 9.45483029e-01\n",
      " 7.18598962e-01 2.51723696e-02 9.43686724e-01 1.19728036e-02\n",
      " 8.22187722e-01 8.81164789e-01 9.37697887e-01 1.13234101e-02\n",
      " 1.01258345e-01 6.91694558e-01 8.75688016e-01 3.42031918e-03\n",
      " 2.81155662e-04 6.56086385e-01 9.98371184e-01 7.13068247e-01\n",
      " 3.99015456e-01 9.77026045e-01 8.09446871e-01 1.03805244e-01\n",
      " 3.27760354e-02 3.47521812e-01 4.28690985e-02 9.88816977e-01\n",
      " 9.56202984e-01 7.60134459e-01 9.58104908e-01 9.34770107e-01\n",
      " 9.86346543e-01 9.85300958e-01 9.86423671e-01 7.61366010e-01\n",
      " 2.44177785e-02 5.76075399e-03 9.87281978e-01 8.80508840e-01\n",
      " 3.12321335e-01 8.98467481e-01 7.87587166e-02 2.00901851e-01\n",
      " 8.32762849e-03 7.16753185e-01 9.31727767e-01 9.96261418e-01\n",
      " 8.02045405e-01 9.72001791e-01 9.85815287e-01 9.97603953e-01\n",
      " 9.82762158e-01 1.18901313e-03 9.61986601e-01 4.69642654e-02\n",
      " 6.73580706e-01 9.85549986e-01 8.34771097e-01 9.79681373e-01\n",
      " 8.16302419e-01 4.71146181e-02 2.68982232e-01 9.45206344e-01\n",
      " 7.23657385e-02 8.92782450e-01 9.68170106e-01 9.79537666e-01\n",
      " 7.74308980e-01 9.77719009e-01 4.13272181e-04 9.78610575e-01\n",
      " 3.08532745e-01 9.79242325e-01 1.30695784e-02 3.66731077e-01\n",
      " 1.12943528e-02 5.82074039e-02 7.18288124e-01 9.25570965e-01\n",
      " 6.58798814e-01 1.27798691e-01 9.90407407e-01 9.53172505e-01\n",
      " 1.16468407e-02 8.72052833e-02 9.97352242e-01 6.95740759e-01\n",
      " 9.38594937e-01 1.48573630e-02 7.45269060e-01 8.58005762e-01\n",
      " 2.67505676e-01 8.36150527e-01 9.80846822e-01 8.30463842e-02\n",
      " 5.03551602e-01 8.73716772e-01 1.89641029e-01 9.87344682e-01\n",
      " 8.04890871e-01 3.85585904e-01 9.30537641e-01 7.40101159e-01\n",
      " 7.99724519e-01 9.89123821e-01 1.25963977e-02 9.98665810e-01\n",
      " 9.83755589e-01 9.96246636e-01 9.95526612e-01 9.79769349e-01\n",
      " 9.78601336e-01 7.83267915e-01 8.13535988e-01 9.91872132e-01\n",
      " 7.33204961e-01 5.16852915e-01 9.39909518e-01 1.25989951e-02\n",
      " 6.14424050e-01 9.78689015e-01 4.97833312e-01 9.18744445e-01\n",
      " 3.05365354e-01 2.09832545e-02 9.79582667e-01 9.01872098e-01\n",
      " 8.19166958e-01 6.37732625e-01 2.87546456e-04 7.76649058e-01\n",
      " 8.12841475e-01 8.45536411e-01 4.38850932e-02 3.74543406e-02\n",
      " 9.90768611e-01 9.06812012e-01 9.58045900e-01 9.20556605e-01\n",
      " 1.96767345e-01 7.05803514e-01 2.20815420e-01 1.44338191e-01\n",
      " 1.29242882e-01 9.94721055e-01 9.81893182e-01 9.28641498e-01\n",
      " 6.23261571e-01 9.28121567e-01 3.61789763e-01 9.26204383e-01\n",
      " 1.43997312e-01 3.52983862e-01 9.86715734e-01 8.62385392e-01\n",
      " 8.79798783e-04 9.05370712e-01 1.19370781e-01 9.32597816e-01\n",
      " 3.64523232e-01 1.44742811e-02 2.83866143e-03 8.96268487e-01\n",
      " 1.97809473e-01 9.99279916e-01 9.97472465e-01 8.72569561e-01\n",
      " 9.91725445e-01 4.11356002e-01 9.64410901e-01 9.15689945e-01\n",
      " 3.33857983e-01 3.04869539e-03 9.86686468e-01 9.48681474e-01\n",
      " 4.55524772e-01 8.32575202e-01 9.07959521e-01 5.65845728e-01\n",
      " 9.77658033e-01 9.78596091e-01 2.85216365e-02 9.06672701e-02\n",
      " 9.87600088e-01 8.26212823e-01 8.83736312e-01 4.11580890e-01\n",
      " 8.40493858e-01 1.28996326e-02 6.27148971e-02 9.50996161e-01\n",
      " 2.04195548e-02 1.62523434e-01 9.97568190e-01 7.60560215e-01\n",
      " 9.91848826e-01 9.93124425e-01 9.96935248e-01 9.47438598e-01\n",
      " 7.72767186e-01 9.44676280e-01 9.98767138e-01 7.31950104e-02\n",
      " 5.33942394e-02 9.85054970e-01 7.83998370e-01 4.23350066e-01\n",
      " 3.37663889e-02 3.45510920e-03 3.59779567e-01 1.47336870e-01\n",
      " 3.92974347e-01 6.03995800e-01 9.77281034e-01 9.17875350e-01\n",
      " 7.65435576e-01 4.61860806e-01 8.56392443e-01 8.89837563e-01\n",
      " 4.84408140e-02 8.71516824e-01 1.67912647e-01 4.30078804e-01\n",
      " 9.13501143e-01 6.14147961e-01 9.92557168e-01 7.67107606e-01\n",
      " 1.63124001e-03 9.90977526e-01 4.63430956e-02 9.77890730e-01\n",
      " 9.98551190e-01 1.35084867e-01 8.99248004e-01 9.93899226e-01\n",
      " 1.27491066e-02 4.03045624e-01 9.95932519e-01 6.89052641e-01\n",
      " 9.96037602e-01 2.90563792e-01 9.91277337e-01 6.92856684e-03\n",
      " 5.90908289e-01 6.25990272e-01 9.52744663e-01 7.13436127e-01\n",
      " 4.50135134e-02 7.82983065e-01 1.33167207e-03 7.00595617e-01\n",
      " 6.68580472e-01 5.06467164e-01 3.84316524e-03 9.54880953e-01\n",
      " 2.39154100e-02 7.70996273e-01 1.14315026e-01 9.73697782e-01\n",
      " 9.89753962e-01 9.96227026e-01 7.64452636e-01 8.37013006e-01\n",
      " 9.39673722e-01 2.13918433e-01 6.99288428e-01 9.43056464e-01\n",
      " 9.94673312e-01 8.95603746e-02 3.63106847e-01 9.94573295e-01\n",
      " 8.71767223e-01 1.76985979e-01 8.47997904e-01 8.81391585e-01\n",
      " 1.69194087e-01 9.97584462e-01 5.72242320e-01 7.81511664e-01\n",
      " 2.02550054e-01 9.42291021e-01 7.74680674e-01 9.69598234e-01\n",
      " 9.57411826e-01 9.15953279e-01 3.38152975e-01 6.90269291e-01\n",
      " 3.66864860e-01 7.28974283e-01 1.53352618e-01 1.09987333e-03\n",
      " 6.95040882e-01 5.52286386e-01 9.90981758e-01 3.71308953e-01\n",
      " 9.77064729e-01 2.55892742e-02 5.80205142e-01 9.74253833e-01\n",
      " 2.24473309e-02 2.16086693e-02 9.16555058e-03 6.35342658e-01\n",
      " 5.77720720e-03 6.78847209e-02 1.94332242e-01 6.11985385e-01\n",
      " 7.62603045e-01 1.52818576e-01 3.49301815e-01 5.38590074e-01\n",
      " 1.21860445e-01 5.86770251e-02 1.47892572e-02 5.80483854e-01\n",
      " 6.52124465e-01 9.91161227e-01 9.88232076e-01 2.67588235e-02\n",
      " 9.28903401e-01 9.07980442e-01 9.82258320e-01 8.99615347e-01\n",
      " 6.47829413e-01 7.83643603e-01 8.19734991e-01 6.50145411e-01\n",
      " 3.67837191e-01 9.96179223e-01 4.72609073e-01 2.77474355e-02\n",
      " 3.37261140e-01 9.61157203e-01 9.80565429e-01 9.98004019e-01\n",
      " 1.23019971e-01 9.84763920e-01 6.18527949e-01 9.77362335e-01\n",
      " 9.83440876e-01 4.75787371e-01 8.78752291e-01 9.97748673e-01\n",
      " 9.35765207e-01 9.07109737e-01 9.90437686e-01 1.99625622e-02\n",
      " 9.39813614e-01 3.57636362e-02 9.88688767e-01 9.79840755e-01\n",
      " 2.57736862e-01 4.38376218e-01 9.06344116e-01 3.04660261e-01\n",
      " 9.61227894e-01 9.37662184e-01 2.34166786e-01 1.24589158e-02\n",
      " 9.74725783e-01 9.69769955e-01 9.38388348e-01 7.69536972e-01\n",
      " 5.72302878e-01 9.82106745e-01 9.69728231e-01 9.07980740e-01\n",
      " 9.95329022e-01 9.53151584e-01 9.88608241e-01 4.49045002e-03\n",
      " 9.99454200e-01 8.95935953e-01 7.14114726e-01 3.43215048e-01\n",
      " 9.54486668e-01 8.59963596e-01 5.72083533e-01 9.95112598e-01\n",
      " 8.73843014e-01 5.24225505e-03 9.95149553e-01 4.26070504e-02\n",
      " 9.52493668e-01 9.98647749e-01 9.76442873e-01 9.82434332e-01\n",
      " 8.87838483e-01 7.78657923e-05 4.33343872e-02 9.93791521e-01\n",
      " 5.54267228e-01 2.14549117e-02 8.01787600e-02 9.65460598e-01\n",
      " 8.74417782e-01 9.60422218e-01 9.28606510e-01 8.40963542e-01\n",
      " 8.69545043e-02 9.78281796e-01 3.12793136e-01 1.40226901e-01\n",
      " 6.22374773e-01 8.90928879e-03 9.56821680e-01 9.64078844e-01\n",
      " 9.97589469e-01 8.73095274e-01 9.78436649e-01 8.87506008e-01\n",
      " 4.32497449e-03 9.74556029e-01 1.44129023e-01 7.03939259e-01\n",
      " 8.07459414e-01 9.83101189e-01 6.90162182e-03 8.10754299e-01\n",
      " 8.58706951e-01 2.64385700e-01 9.57234442e-01 9.75102484e-01\n",
      " 7.45569408e-01 3.61398445e-03 9.79895949e-01 6.19049847e-01\n",
      " 6.15805924e-01 7.38448739e-01 7.50407636e-01 2.99389035e-01\n",
      " 1.61577633e-03 9.19137955e-01 8.99211913e-02 9.45643544e-01\n",
      " 6.24131262e-01 4.89052951e-01 4.16542321e-01 4.57355946e-01\n",
      " 7.71247447e-01 3.21800064e-04 9.42514360e-01 9.86116946e-01\n",
      " 9.23286080e-01 6.55611098e-01 9.68002379e-01 8.16295743e-01\n",
      " 9.69361007e-01 9.12019551e-01 9.96217668e-01 9.01805639e-01\n",
      " 9.37992215e-01 9.78511631e-01 2.86187828e-01 4.59018290e-01\n",
      " 6.06112659e-01 2.31605619e-02 9.46310282e-01 3.32208306e-01\n",
      " 9.47943807e-01 6.27434289e-04 7.40072310e-01 5.07979214e-01\n",
      " 9.98605430e-01 9.93002355e-01 9.33178246e-01 9.32728946e-01\n",
      " 8.57968032e-01 9.50364053e-01 9.74793077e-01 1.88139409e-01\n",
      " 9.85937417e-01 4.92842272e-02 9.77977037e-01 6.51997654e-03\n",
      " 9.97601569e-01 5.17324544e-02 9.88905191e-01 1.08333841e-01\n",
      " 7.98695922e-01 4.39312488e-01 9.60802257e-01 8.82719159e-01\n",
      " 6.18966222e-01 6.80385008e-02 9.64294493e-01 9.71728802e-01\n",
      " 3.14516991e-01 9.74119782e-01 9.38916862e-01 9.95471954e-01\n",
      " 6.69406414e-01 8.67226720e-01 9.92579401e-01 9.94863570e-01\n",
      " 2.93544983e-03 9.94998336e-01 4.35760170e-01 9.06589270e-01\n",
      " 9.40825999e-01 8.86973321e-01 3.97956610e-01 1.32103577e-01\n",
      " 9.88403916e-01 9.58701372e-01 9.81740475e-01 2.90923808e-02\n",
      " 9.97106373e-01 2.11280435e-01 9.58124578e-01 9.96646464e-01\n",
      " 2.45622848e-03 9.93296683e-01 1.08044386e-01 3.29100788e-02\n",
      " 1.01560719e-01 1.80985767e-03 1.26738608e-01 9.95654941e-01\n",
      " 8.84701252e-01 5.67427218e-01 8.92227352e-01 4.73118037e-01\n",
      " 2.22924232e-01 5.76647371e-02 4.65868562e-02 9.87346232e-01\n",
      " 9.76082802e-01 5.02719820e-01 5.28764248e-01 9.84985709e-01\n",
      " 6.30842626e-01 1.28521875e-01 3.72975349e-01 9.91622508e-01\n",
      " 6.36864841e-01 7.09563434e-01 9.68381345e-01 1.32360570e-02\n",
      " 7.89690793e-01 9.88520861e-01 9.21836272e-02 8.88523385e-02\n",
      " 4.28048044e-01 9.97054815e-01 8.86239350e-01 1.45960541e-03\n",
      " 9.48198617e-01 2.50940114e-01 8.70644987e-01 9.21148509e-02\n",
      " 1.93282578e-03 6.13163590e-01 9.62976515e-01 9.84758914e-01\n",
      " 7.05309272e-01 6.20513022e-01 9.89783943e-01 3.36361766e-01\n",
      " 9.37599182e-01 4.89102244e-01 9.98976588e-01 1.26901781e-02\n",
      " 8.07132900e-01 1.43340096e-01 1.74945332e-02 2.38230228e-02\n",
      " 3.88923474e-02 1.60881318e-02 5.70212960e-01 8.46228600e-01\n",
      " 9.74923432e-01 9.63079810e-01 9.21123862e-01 4.23301429e-01\n",
      " 6.21632524e-02 4.32513893e-01 4.00079675e-02 6.39841333e-03\n",
      " 1.55492816e-02 4.03047800e-01 9.36173916e-01 4.08825189e-01\n",
      " 8.86524975e-01 9.88658726e-01 9.69605744e-01 9.27982807e-01\n",
      " 1.05700791e-02 9.64596987e-01 4.74855214e-01 8.77401292e-01\n",
      " 1.23645533e-02 1.60297424e-01 6.42395258e-01 9.22458112e-01\n",
      " 8.31592202e-01 9.51806664e-01 8.13649356e-01 1.36533365e-01\n",
      " 3.80895913e-01 4.91892964e-01 7.91600287e-01 9.46484387e-01\n",
      " 9.85376537e-01 3.62628214e-02 1.95668880e-02 6.96423531e-01\n",
      " 7.59303808e-01 8.41893017e-01 9.79771912e-01 8.84038687e-01\n",
      " 9.74549592e-01 9.82314110e-01 3.69646624e-02 8.02092850e-01\n",
      " 1.44460122e-03 7.04496130e-02 5.48089504e-01 9.17730749e-01\n",
      " 9.21733260e-01 8.25941086e-01 5.68668067e-01 9.51927841e-01\n",
      " 3.56282433e-03 9.22982633e-01 2.77624339e-01 6.47133172e-01\n",
      " 9.82285380e-01 1.11448422e-01 2.63152838e-01 9.68152761e-01\n",
      " 6.31887078e-01 7.73102701e-01 2.30254959e-02 9.17433500e-01\n",
      " 9.96868074e-01 1.30755617e-03 4.44249046e-04 1.79821908e-01\n",
      " 9.34025049e-01 2.61241645e-01 8.63771439e-01 3.03238183e-01\n",
      " 1.09958677e-02 7.88052082e-02 8.23178768e-01 9.94901180e-01\n",
      " 9.86043930e-01 9.56496119e-01 1.84870154e-01 5.84877312e-01\n",
      " 9.26490352e-02 3.15690964e-01 9.90015805e-01 9.47953999e-01\n",
      " 9.80284393e-01 9.32769954e-01 9.65386569e-01 9.81669128e-01\n",
      " 9.78465617e-01 4.79221463e-01 4.06277459e-03 3.07968725e-02\n",
      " 9.76253808e-01 8.20870817e-01 9.68408704e-01 9.52501118e-01\n",
      " 7.02397466e-01 5.29485226e-01 5.78107312e-03 9.88147795e-01\n",
      " 9.61687326e-01 9.98667598e-01 9.95874226e-01 1.49562180e-01\n",
      " 7.04949915e-01 9.79102314e-01 1.27548352e-01 2.36598909e-01\n",
      " 1.71129890e-02 9.93300915e-01 8.68797123e-01 1.24426074e-02\n",
      " 7.06361949e-01 9.95872438e-01 9.77800041e-02 9.88065481e-01\n",
      " 5.53090684e-03 7.04711012e-04 3.69981453e-02 9.01932299e-01\n",
      " 1.10109210e-01]\n"
     ]
    }
   ],
   "source": [
    "# this should be a numpy array of length 941 of probabilities\n",
    "def calculate_self_confidences(pred_probs: np.ndarray, labels: np.ndarray):\n",
    "    assert len(pred_probs) == len(labels)\n",
    "    self_confidences = np.zeros(len(labels))\n",
    "    for i in range(len(labels)):\n",
    "        self_confidences[i] = pred_probs[i, labels[i]]\n",
    "    return self_confidences\n",
    "self_confidences = calculate_self_confidences(pred_probs, labels)\n",
    "print(\"self_confidences\", self_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we rank the _indices_ of the data points by the self-confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be a numpy array of length 941 of integer indices\n",
    "ranked_indices = np.argsort(self_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the indices of label issues as the top `num_label_issues` items in the `ranked_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_idx = ranked_indices[:num_label_issues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of the highest-ranked data points (most likely to be label issues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>77c9c5</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>dacfb9</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>e2614a</td>\n",
       "      <td>85</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b0306d</td>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>347d55</td>\n",
       "      <td>98</td>\n",
       "      <td>51</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stud_ID  exam_1  exam_2  exam_3                       notes letter_grade  \\\n",
       "637  77c9c5       0      79      65  cheated on exam, gets 0pts            F   \n",
       "264  dacfb9      80      60      80                         NaN            C   \n",
       "378  e2614a      85      62      75                         NaN            C   \n",
       "689  b0306d      77      51      70                         NaN            D   \n",
       "318  347d55      98      51      74                         NaN            C   \n",
       "\n",
       "    noisy_letter_grade  \n",
       "637                  A  \n",
       "264                  F  \n",
       "378                  F  \n",
       "689                  B  \n",
       "318                  F  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.iloc[ranked_indices[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvJHkPzSq6Q"
   },
   "source": [
    "# How'd We Do?\n",
    "\n",
    "Let's go a step further and see how we did at automatically identifying which data points are mislabeled. If we take the intersection of the labels errors identified by Confident Learning and the true label errors, we see that our approach was able to identify 75% of the label errors correctly (based on predictions from a model that is only 67% accurate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O2a6urWc1DA",
    "outputId": "f88b5ce6-33f2-4ef6-e774-19013c33f0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of errors found: 76.6%\n"
     ]
    }
   ],
   "source": [
    "# Computing percentage of true errors identified. \n",
    "true_error_idx = df[df.letter_grade != df.noisy_letter_grade].index.values\n",
    "cl_acc = len(set(true_error_idx).intersection(set(issue_idx)))/len(true_error_idx)\n",
    "print(f\"Percentage of errors found: {round(cl_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzxXoDOqSzn-"
   },
   "source": [
    "# Train a More Robust Model\n",
    "\n",
    "Now that we have the indices of potential label errors within our data, let's remove them from our data, retrain our model, and see what improvement we can gain.\n",
    "\n",
    "Keep in mind that our baseline model from above, trained on the original data using the `noisy_letter_grade` as the prediction label, achieved a cross-validation accuracy of 67%.\n",
    "\n",
    "Let's use a very simple method to handle these label errors and just drop them entirely from the data and retrain our exact same `XGBClassifier`. In a real-world application, a better approach might be to have humans review the issues and _correct_ the labels rather than dropping the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsQFmy7xgSUa",
    "outputId": "a33e17ab-c197-4f95-c9c1-0473c16af313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 67.4%\n",
      "Accuracy with errors found by Confident Learning removed: 90.3%\n",
      "Reduction in error: 70.3%\n"
     ]
    }
   ],
   "source": [
    "# Remove the label errors found by Confident Learning\n",
    "data = df.drop(issue_idx)\n",
    "clean_labels = data['noisy_letter_grade']\n",
    "data = data.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "\n",
    "# Train a more robust classifier with less erroneous data\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "clean_pred_probs = cross_val_predict(model, data, clean_labels, method='predict_proba')\n",
    "clean_preds = np.argmax(clean_pred_probs, axis=1)\n",
    "\n",
    "acc_clean = accuracy_score(clean_preds, clean_labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100, 1)}%\")\n",
    "print(f\"Accuracy with errors found by Confident Learning removed: {round(acc_clean*100, 1)}%\")\n",
    "\n",
    "# Compute reduction in error.\n",
    "err = ((1-acc_original)-(1-acc_clean))/(1-acc_original)\n",
    "print(f\"Reduction in error: {round(err*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J9clVf1UzQZ"
   },
   "source": [
    "After removing the suspected label issues, our model's new cross-validation accuracy is now 90%, which means we **reduced the error-rate of the model by 70%** (the original model had 67% accuracy). \n",
    "\n",
    "**Note: throughout this entire process we never changed any code related to model architecture/hyperparameters, training, or data preprocessing!  This improvement is strictly coming from increasing the quality of our data which leaves additional room for additional optimizations on the modeling side.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W-Lo82SVp7I"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "For the student grades dataset, we found that simply dropping identified label errors and retraining the model resulted in a 70% reduction in prediction error on our classification problem (with accuracy improving from 67% to 90%).\n",
    "\n",
    "An implementation of the Confident Learning algorithm (and much more) is available in the [cleanlab](https://github.com/cleanlab/cleanlab) library on GitHub. This is how today's lab assignment can be done in a single line of code with Cleanlab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "\n",
    "cl_issue_idx = cleanlab.filter.find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>77c9c5</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>dacfb9</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>e2614a</td>\n",
       "      <td>85</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b0306d</td>\n",
       "      <td>77</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>347d55</td>\n",
       "      <td>98</td>\n",
       "      <td>51</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stud_ID  exam_1  exam_2  exam_3                       notes letter_grade  \\\n",
       "637  77c9c5       0      79      65  cheated on exam, gets 0pts            F   \n",
       "264  dacfb9      80      60      80                         NaN            C   \n",
       "378  e2614a      85      62      75                         NaN            C   \n",
       "689  b0306d      77      51      70                         NaN            D   \n",
       "318  347d55      98      51      74                         NaN            C   \n",
       "\n",
       "    noisy_letter_grade  \n",
       "637                  A  \n",
       "264                  F  \n",
       "378                  F  \n",
       "689                  B  \n",
       "318                  F  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.iloc[cl_issue_idx[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Advanced topic_: you might notice that the above `cl_issue_idx` differs in length (by a little bit) from our `issue_idx`. The reason for this is that we implemented a slightly simplified version of the algorithm in this lab. We skipped a calibration step after computing the confident joint that makes the confident joint have the true noisy prior $p(labels)$ (summed over columns for each row) and also add up to the total number of examples. If you're interested in the details of this, see equation 3 and the subsequent explanation in the [paper](https://jair.org/index.php/jair/article/view/12125)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
